---
title: "Cost Monitoring"
description: "Understanding and managing the costs associated with Large Language Model (LLM) usage is a critical aspect of operating an AI platform. Peak RAG provides a dedicated metrics dashboard to give you v..."
---

# Cost Monitoring

Understanding and managing the costs associated with Large Language Model (LLM) usage is a critical aspect of operating an AI platform. Peak RAG provides a dedicated metrics dashboard to give you visibility into how costs are incurred. This document explains how those costs are calculated and what the metrics represent.

## How Costs are Calculated

The primary driver of cost is **token usage**. Tokens are the basic units of text that LLMs process (roughly, one token is about 4 characters or 0.75 words). AI providers like OpenAI and Azure charge based on the number of tokens processed in both the input (prompt) and the output (completion).

The Peak RAG backend meticulously tracks the token count for every LLM call made by an agent.

### The Cost Calculation Formula

For each agent interaction, the cost is calculated as follows:

```
TotalCost = (PromptTokens / 1,000,000 * PromptCostPerMillion) + (CompletionTokens / 1,000,000 * CompletionCostPerMillion)
```

- **`PromptTokens`**: The number of tokens in the prompt sent to the LLM. This includes the system prompt, any injected definitions, the conversation history, the retrieved knowledge base context, and the user's query.
- **`CompletionTokens`**: The number of tokens in the response generated by the LLM.
- **`CostPerMillion`**: The pricing rate for the specific LLM being used (e.g., `gpt-4o-mini`, `gpt-4-turbo`). These rates are configured in the backend and vary significantly between models.

All of this data is stored in the PostgreSQL database for each message, allowing for detailed cost analysis.

## Metrics Dashboard Explained

The metrics dashboard in the application aggregates this raw data to provide actionable insights.

### Key Metric Cards

- **Avg Cost / Call**:

  - **What it is**: The average cost of a single, complete agent response (a "call").
  - **Calculation**: `SUM(TotalCost of all messages) / COUNT(total number of agent messages)`.
  - **Why it matters**: This is your most important efficiency metric. A low average cost per call indicates that your agents are resolving queries efficiently. Spikes in this metric can indicate issues like overly long prompts, inefficient retrieval, or users asking overly complex questions.

- **Avg Prompt Tokens / Call**:

  - **What it is**: The average number of tokens being sent _to_ the LLM in the prompt.
  - **Calculation**: `SUM(PromptTokens of all messages) / COUNT(total number of agent messages)`.
  - **Why it matters**: This helps you understand the main drivers of your input costs. High prompt tokens are often caused by:
    - Very long and detailed system prompts.
    - A high `Result Limit` in the Knowledge Retrieval tool, which stuffs the prompt with a lot of context.
    - Long conversation histories being maintained.

- **Avg Completion Tokens / Call**:
  - **What it is**: The average number of tokens being generated _by_ the LLM in its response.
  - **Calculation**: `SUM(CompletionTokens of all messages) / COUNT(total number of agent messages)`.
  - **Why it matters**: This reflects the verbosity of your agents. If this number is too high, it may mean your system prompt is not instructing the agent to be concise, leading to unnecessarily long and expensive answers.

### Charts

- **Costs Chart**: A time-series visualization of the total cumulative cost per day. This helps you track your overall spend and identify trends.
- **Calls Chart**: A time-series visualization of the total number of agent calls per day. This shows user engagement and helps contextualize the cost data. An increase in costs is expected if it correlates with an increase in calls.

